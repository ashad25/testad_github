{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1 - Splitting data into training and validation datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In this step, you import the universe dataset which is a csv file now,  using a python library called pandas. One can press Shift key and enter to run.  Note that here in most of the statements where there is an output a print command is not needed only for notebook.  <To insert a row use a or b and also type m for markdown/comments. >  delete a line dd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "#pandas is used to import/export/manipulate data\n",
    "from sklearn.model_selection import train_test_split\n",
    "#sklearn(Sci-Kit Learn) has a function called train_test_split. It is used to split original data set\n",
    "#to training/validation dataset.\n",
    "#sklearn also has many machine learning algorithms such as Support Vector Machine, Random Forrest as well as \n",
    "#Logistic Regression\n",
    "#sys is a python library you use when you need to import helper functions that are saved in different\n",
    "import sys\n",
    "#sys is used to import codes saved in aother folder\n",
    "StandardRecodes = r'U:\\Research\\TensorFlow\\Standard Recodes'\n",
    "sys.path.insert(0, StandardRecodes)\n",
    "import SAS_Macros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We import the following- CSV file data, Featurization , and Standard recodes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here we are creating a training and validation datasets\n",
    "#path to the folder where original data set is saved\n",
    "PIC= r'U:\\Research\\TensorFlow\\Data\\CSV'\n",
    "#path to the folder where txt file that contains all the variables name is saved\n",
    "feature = r'U:\\Research\\TensorFlow\\Standard Recodes\\Feature Selection'\n",
    "#path to the folder where txt file that contains all the variables name is saved\n",
    "output = r'U:\\Research\\TensorFlow\\Output'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing the original dataset. \n",
    "#usecols is used to not import unneccesary data elements\n",
    "#df stands for DataFrame, which is a data type of python, specifically pandas nothing but contains rows and columns\n",
    "#It is basically a way to store tabular data where you can label the rows and the columns. \n",
    "df_Universe = pd.read_csv(PIC + r'\\PIC_Universe.csv', low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MRI_ID</th>\n",
       "      <th>resp</th>\n",
       "      <th>hhld_member_count</th>\n",
       "      <th>location_type</th>\n",
       "      <th>downgrade_reason_code</th>\n",
       "      <th>move_out_ind</th>\n",
       "      <th>health_contrib_ind</th>\n",
       "      <th>political_contrib_ind</th>\n",
       "      <th>religious_contrib_ind</th>\n",
       "      <th>donor_ever</th>\n",
       "      <th>...</th>\n",
       "      <th>lot_size_acres_bin</th>\n",
       "      <th>Sale_Price_Filled</th>\n",
       "      <th>sale_prc_div_1000_Bin</th>\n",
       "      <th>Interest_Rate_filled</th>\n",
       "      <th>interest_Rate_Bin</th>\n",
       "      <th>loan_amount_filled</th>\n",
       "      <th>loan_amount_zero</th>\n",
       "      <th>loan_amount_bin</th>\n",
       "      <th>Market_Value_Filled</th>\n",
       "      <th>Market_Value_Bin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6900100102</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>S</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>169.00000</td>\n",
       "      <td>7.0</td>\n",
       "      <td>5.63</td>\n",
       "      <td>4.0</td>\n",
       "      <td>161.0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>127.000000</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6900100103</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>S</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>189.87186</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.38</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>183.583266</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6900100104</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>S</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>189.87186</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.09</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>183.583266</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6900100105</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>S</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>88.00000</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.71</td>\n",
       "      <td>1.0</td>\n",
       "      <td>83.0</td>\n",
       "      <td>0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>113.000000</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6900100108</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>S</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>75.00000</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.77</td>\n",
       "      <td>5.0</td>\n",
       "      <td>105.0</td>\n",
       "      <td>0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>127.000000</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 821 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       MRI_ID  resp  hhld_member_count location_type  downgrade_reason_code  \\\n",
       "0  6900100102     0                  5             S                    NaN   \n",
       "1  6900100103     0                  5             S                    NaN   \n",
       "2  6900100104     0                  2             S                    NaN   \n",
       "3  6900100105     0                  1             S                    NaN   \n",
       "4  6900100108     0                  7             S                    NaN   \n",
       "\n",
       "   move_out_ind  health_contrib_ind  political_contrib_ind  \\\n",
       "0             0                   0                      0   \n",
       "1             0                   0                      0   \n",
       "2             0                   0                      0   \n",
       "3             0                   0                      0   \n",
       "4             0                   0                      0   \n",
       "\n",
       "   religious_contrib_ind  donor_ever  ...  lot_size_acres_bin  \\\n",
       "0                      0           0  ...                 1.0   \n",
       "1                      0           0  ...                 0.0   \n",
       "2                      0           0  ...                 0.0   \n",
       "3                      0           0  ...                 1.0   \n",
       "4                      0           0  ...                 2.0   \n",
       "\n",
       "   Sale_Price_Filled  sale_prc_div_1000_Bin  Interest_Rate_filled  \\\n",
       "0          169.00000                    7.0                  5.63   \n",
       "1          189.87186                    0.0                  6.38   \n",
       "2          189.87186                    0.0                  6.09   \n",
       "3           88.00000                    4.0                  3.71   \n",
       "4           75.00000                    3.0                  5.77   \n",
       "\n",
       "   interest_Rate_Bin  loan_amount_filled loan_amount_zero  loan_amount_bin  \\\n",
       "0                4.0               161.0                0              7.0   \n",
       "1                8.0                 0.0                1              0.0   \n",
       "2                6.0                 0.0                1              0.0   \n",
       "3                1.0                83.0                0              3.0   \n",
       "4                5.0               105.0                0              5.0   \n",
       "\n",
       "   Market_Value_Filled  Market_Value_Bin  \n",
       "0           127.000000               6.0  \n",
       "1           183.583266               NaN  \n",
       "2           183.583266               NaN  \n",
       "3           113.000000               5.0  \n",
       "4           127.000000               6.0  \n",
       "\n",
       "[5 rows x 821 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_Universe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is just to makes sure that there are no duplicate fields. we are reading the last 2 bytes of col name. \n",
    "for var in sorted(list(df_Universe.columns)):\n",
    "    if var[-2:] == '.1':\n",
    "        print (var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Division(x):\n",
    "    if x in ['ME','NH','VT','MA','RI','CT']:\n",
    "        return 'New England'\n",
    "    elif x in ['NY','NJ','PA']:\n",
    "        return 'Mid ATL'\n",
    "    elif x in ['OH','IN','IL','MI','WI']:\n",
    "        return 'E N Central'\n",
    "    elif x in ['MN','IA','MO','ND','SD','NE','KS']:\n",
    "        return 'W N Central'\n",
    "    elif x in ['DE','MD','DC','VA','WV','NC','SC','GA','FL']:\n",
    "        return 'S ATL'\n",
    "    elif x in ['KY','TN','AL','MS']:\n",
    "        return 'E S Central'\n",
    "    elif x in ['AR','LA','OK','TX']:\n",
    "        return 'W S Central'\n",
    "    elif x in ['MT','ID','WY','CO','NM','AZ','UT','NV']:\n",
    "        return 'Mountain'\n",
    "    elif x in ['WA','OR','CA','AK','HI']:\n",
    "        return 'Pacific'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Applying the function created above on 'state_abbr' to create a new field 'Division'\n",
    "#lambda functions are small functions usually not more than a line. It can have any number of arguments\n",
    "#just like a normal function. \n",
    "df_Universe['Division'] = df_Universe.state_abbr.apply(lambda x: Division(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing a text file\n",
    "#In PIC model, we do not use race, gender, and ethnicity. We also do not use fields that were built after PIC.\n",
    "with open(feature + r'\\Vars_to_Remove.txt', 'r') as f:\n",
    "    Vars_to_Remove = f.read().split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in Vars_to_Remove:\n",
    "    try:\n",
    "        #If you want to delete an entire column run the following\n",
    "        #Syntax: pd.DataFrame.drop([column name], axis = 1(meaning column), \n",
    "        #                          inplace = True(or it won't actuallt drop the column))\n",
    "        df_Universe.drop([col], axis = 1, inplace = True)\n",
    "        print (col)\n",
    "    except:\n",
    "        pass\n",
    "#try - except\n",
    "#In the above case, if the column you tried to remove did not exist in the DataFrame, it would raise an error.\n",
    "#In case of error, command in the except is executed instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting original dataset into training/validation datasets\n",
    "#train_test_split is a function in sklearn which splits a DataFrame into train and test datasets. random state=42 \n",
    "#it is a seed\n",
    "# x - Estimators\n",
    "# y - target\n",
    "#Y_model and Y_valid is more a dummy dataset created in order for the split to work. Note that it only has 1 variable \n",
    "#which is the response. The x_MODEL AND X_VALID still contains the target variable resp in it. \n",
    "X_model, X_validation, y_model, y_validation = train_test_split(df_Universe, \n",
    "                                                                df_Universe['resp'], \n",
    "                                                                test_size = 0.5,\n",
    "                                                                random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    18901\n",
       "1      648\n",
       "Name: resp, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_model.resp.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    18919\n",
       "1      631\n",
       "Name: resp, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_validation.resp.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MRI_ID</th>\n",
       "      <th>resp</th>\n",
       "      <th>hhld_member_count</th>\n",
       "      <th>location_type</th>\n",
       "      <th>downgrade_reason_code</th>\n",
       "      <th>move_out_ind</th>\n",
       "      <th>health_contrib_ind</th>\n",
       "      <th>political_contrib_ind</th>\n",
       "      <th>religious_contrib_ind</th>\n",
       "      <th>donor_ever</th>\n",
       "      <th>...</th>\n",
       "      <th>Sale_Price_Filled</th>\n",
       "      <th>sale_prc_div_1000_Bin</th>\n",
       "      <th>Interest_Rate_filled</th>\n",
       "      <th>interest_Rate_Bin</th>\n",
       "      <th>loan_amount_filled</th>\n",
       "      <th>loan_amount_zero</th>\n",
       "      <th>loan_amount_bin</th>\n",
       "      <th>Market_Value_Filled</th>\n",
       "      <th>Market_Value_Bin</th>\n",
       "      <th>Division</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>22241</th>\n",
       "      <td>7100452503</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>S</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>77.00000</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.72</td>\n",
       "      <td>5.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>183.583266</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Mid ATL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34050</th>\n",
       "      <td>7201022209</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>S</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>189.87186</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.00</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>183.583266</td>\n",
       "      <td>NaN</td>\n",
       "      <td>W S Central</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33597</th>\n",
       "      <td>7200913601</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>S</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>189.87186</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.00</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>183.583266</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S ATL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25217</th>\n",
       "      <td>7111615304</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>S</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>189.87186</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.58</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>183.583266</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S ATL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25786</th>\n",
       "      <td>7112802712</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>S</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>69.00000</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6.32</td>\n",
       "      <td>8.0</td>\n",
       "      <td>122.0</td>\n",
       "      <td>0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>130.000000</td>\n",
       "      <td>6.0</td>\n",
       "      <td>E N Central</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 822 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           MRI_ID  resp  hhld_member_count location_type  \\\n",
       "22241  7100452503     0                  5             S   \n",
       "34050  7201022209     0                  1             S   \n",
       "33597  7200913601     0                  2             S   \n",
       "25217  7111615304     0                  2             S   \n",
       "25786  7112802712     0                  3             S   \n",
       "\n",
       "       downgrade_reason_code  move_out_ind  health_contrib_ind  \\\n",
       "22241                    NaN             0                   0   \n",
       "34050                    NaN             0                   0   \n",
       "33597                    NaN             0                   0   \n",
       "25217                    NaN             0                   0   \n",
       "25786                    NaN             0                   0   \n",
       "\n",
       "       political_contrib_ind  religious_contrib_ind  donor_ever  ...  \\\n",
       "22241                      0                      0           1  ...   \n",
       "34050                      0                      0           0  ...   \n",
       "33597                      0                      0           0  ...   \n",
       "25217                      0                      0           0  ...   \n",
       "25786                      0                      0           1  ...   \n",
       "\n",
       "       Sale_Price_Filled  sale_prc_div_1000_Bin  Interest_Rate_filled  \\\n",
       "22241           77.00000                    3.0                  5.72   \n",
       "34050          189.87186                    0.0                  5.00   \n",
       "33597          189.87186                    0.0                  5.00   \n",
       "25217          189.87186                    0.0                  5.58   \n",
       "25786           69.00000                    3.0                  6.32   \n",
       "\n",
       "       interest_Rate_Bin  loan_amount_filled  loan_amount_zero  \\\n",
       "22241                5.0                67.0                 0   \n",
       "34050                3.0                 0.0                 1   \n",
       "33597                3.0                 0.0                 1   \n",
       "25217                4.0                 0.0                 1   \n",
       "25786                8.0               122.0                 0   \n",
       "\n",
       "      loan_amount_bin  Market_Value_Filled  Market_Value_Bin     Division  \n",
       "22241             3.0           183.583266               NaN      Mid ATL  \n",
       "34050             0.0           183.583266               NaN  W S Central  \n",
       "33597             0.0           183.583266               NaN        S ATL  \n",
       "25217             0.0           183.583266               NaN        S ATL  \n",
       "25786             5.0           130.000000               6.0  E N Central  \n",
       "\n",
       "[5 rows x 822 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_model.head()# note it contains resp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "for var in sorted(list(X_model.columns)):\n",
    "    if var[-2:] == '.1':\n",
    "        print (var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dataset = PIC + r'\\df_Model.csv'\n",
    "X_model.to_csv(model_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_validation.to_csv(PIC + r'\\df_validation.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other extensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "other_data_types = r'U:\\Research\\TensorFlow\\Data\\others'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install pyarrow\n",
    "X_model.to_parquet(other_data_types + r'\\df_model.parquet', engine = 'pyarrow')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#If you want to do SAS equivalent of proc univariate, use .describe() only for numeric vars. \n",
    "df_Universe[['resp', 'find_div_1000','LHI_Auto', 'HOH_Catholic']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if you want proc univariate on all and then export it as a csv.\n",
    "df_Universe.describe().to_csv(output + r'\\universe_univariate.csv')\n",
    "#output = r'U:\\Research\\TensorFlow\\Output'\n",
    "#output + r'\\universe_univariate.csv' = r'U:\\Research\\TensorFlow\\Output\\universe_univariate.csv'\n",
    "#and thats where the csv file is saved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#You can also do the same for one variable\n",
    "df_Universe.find_div_1000.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#If you only want to know mean,\n",
    "df_Universe.find_div_1000.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Crosstab using pandas(pd)\n",
    "pd.crosstab(df_Universe.resp, df_Universe.FINDGE90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SAS_Macros is a library created which has functions such as crosstab.\n",
    "#Syntax : SAS_Macros.crosstab(DataFrame, first variable name, second variable name)\n",
    "SAS_Macros.crosstab(df_Universe, 'resp', 'FINDGE90')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#If you want to know data type of a variable, use type()\n",
    "type(df_Universe)\n",
    "#Data type for df_Universe is pandas.core.frame.DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#What is series? It is another data type of pandas for one var. \n",
    "type(df_Universe.resp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To run a few variables counts - non missing counts and datatypes \n",
    "df_Universe[['resp','find_div_1000','LHI_Auto']].info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#You can view all the variables by using a for loop and here the vars are sorted. \n",
    "for var in sorted(list(df_Universe.columns)):\n",
    "    print (var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#You can view all the variables by using a for loop, here the vars are not sorted and also the sequence of vars are listed\n",
    "i = 0\n",
    "for var in (list(df_Universe.columns)):\n",
    "    print (i, var)\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a for Loop to run multiple variable freq. \n",
    "for var in ['resp', 'HOH_Catholic', 'LHI_Auto']:\n",
    "    print (var)\n",
    "    print (df_Universe[var].value_counts())\n",
    "    print ('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#You can sort by .sort_value(by = 'variabl name')\n",
    "df_Universe.sort_values(by = 'Age0_11TotalHH').head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# However, the code above did not actually sort the dataset. Here again printing a few records. \n",
    "df_Universe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#In order to actually sort the original dataset, you need to pass inplace = True which then sorts the original\n",
    "#dataset which sorts the df_universe\n",
    "df_Universe.sort_values(by = 'Age0_11TotalHH', inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now it is actually sorted\n",
    "df_Universe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1 + 'a'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#When there is something wrong with yoour code, you will see an Error message like above. \n",
    "#If you didn't see an error message, that means your code ran without a problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sharp(#) is used to comment out in python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Universe.loc[:,'AvgAge']# here we list all the rows in AvgAge. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Universe.iloc[0,:] #Iloc is index based vars, here we have row #1 and all col's "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Standard Deletes\n",
    "#If yu want to remove records with missing LHI_Auto\n",
    "df_Universe = df_Universe[df_Universe.LHI_Auto == df_Universe.LHI_Auto]\n",
    "#The logic behind is creating a subset of df_Universe whose LHI_Auto is not missing, and assign the same name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Universe.LHI_Auto.unique()\n",
    "#No missing now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_Universe.donor_ever.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#You could have multiple condisions too\n",
    "df_Universe = df_Universe[(df_Universe.health_contrib_ind == 0) & \n",
    "                          (df_Universe.donor_ever == 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Or is |\n",
    "df_Universe = df_Universe[(df_Universe.mob_ever != 0) | \n",
    "                          (df_Universe.mail_resp_ever_ind == 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
